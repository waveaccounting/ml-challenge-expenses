{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifcation using Spark\n",
    "\n",
    "This notebook tries to address the following problem:\n",
    "\n",
    "*(Bonus) Train your learning algorithm for one of the above questions in a distributed fashion, such as using Spark. Here, you can assume either the data or the model is too large/efficient to be process in a single computer.*\n",
    "\n",
    "\n",
    "Although in the original solution I tried different classifiers and ensemble, int his spark based one, I am only using RandomForest for simplicity. I don't have too much exposure to Spark's machine learning libraries, so this ia quite a bit of an exploration for me.\n",
    "\n",
    "Although here I intended to generate all features (text and non-text), for limited time, I will finally be using only the text features for classification purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import calendar\n",
    "import numpy as np\n",
    "    \n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate sark session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read n the data in datafarames and create temp views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.format('csv').options(header='true', inferschema='true').load('training_data_example.csv')\n",
    "\n",
    "df_val = spark.read.format('csv').options(header='true', inferschema='true').load('validation_data_example.csv')\n",
    "\n",
    "df_employee = spark.read.format('csv').options(header='true', inferschema='true').load('employee.csv')\n",
    "df_employee = df_employee.withColumnRenamed('employee id', 'employee_id')\n",
    "df_employee.createOrReplaceTempView('employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lencoder = LabelEncoder()\n",
    "lencoder.fit(df_train.select('category').rdd.map(lambda x: x[0]).collect())\n",
    "names = set(df_val.select('category').rdd.map(lambda x: x[0]).collect()) # label names to be used later\n",
    "y_train = lencoder.transform(df_train.select('category').rdd.map(lambda x: x[0]).collect())\n",
    "y_val = lencoder.transform(df_val.select('category').rdd.map(lambda x: x[0]).collect())\n",
    "val_categoroes = []\n",
    "for clazz in lencoder.classes_:\n",
    "    if clazz in names:\n",
    "        val_categoroes.append(clazz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define some UDFs to use in spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_month(date)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weekday(date):\n",
    "    month, day, year = (int(x) for x in date.split('/'))    \n",
    "    weekday = datetime.date(year, month, day)\n",
    "    return calendar.day_name[weekday.weekday()]\n",
    "\n",
    "def get_month(date):\n",
    "    month, day, year = (int(x) for x in date.split('/'))    \n",
    "    return month\n",
    "\n",
    "spark.udf.register('get_weekday', get_weekday)\n",
    "spark.udf.register('get_month', get_month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next define some tokenizer and vectorizer for the TF-IDF vectorization purpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"expense_description\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a couple of methods for getting a bit more formatted features and vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df, table):\n",
    "    df = df.withColumnRenamed('employee id', 'employee_id') \\\n",
    "         .withColumnRenamed('expense description', 'expense_description') \\\n",
    "         .withColumnRenamed('pre-tax amount', 'pre_tax_amount') \\\n",
    "         .withColumnRenamed('tax amount', 'tax_amount') \\\n",
    "         .withColumnRenamed('tax name', 'tax_name')\n",
    "            \n",
    "    df.createOrReplaceTempView(table)\n",
    "    \n",
    "    df = spark.sql(\"\"\"\n",
    "        select employee_id,\n",
    "           get_weekday(date) as weekday,\n",
    "           cast(get_month(date) as int) as month,\n",
    "           pre_tax_amount,\n",
    "           role,\n",
    "           expense_description,\n",
    "           case when category = 'Computer - Hardware' then 0\n",
    "               when category = 'Computer - Software'  then 1\n",
    "               when category = 'Meals and Entertainment' then 2\n",
    "               when category = 'Office Supplies' then 3\n",
    "               else 4\n",
    "           end as category\n",
    "        from \n",
    "        {table} \n",
    "        inner join employee using(employee_id)\n",
    "    \n",
    "    \"\"\".format(table=table))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vectorize(df):\n",
    "    wordsData = tokenizer.transform(df)\n",
    "    \n",
    "    featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "    rescaledData = rescaledData.select(\"features\", \"category\")\n",
    "    return rescaledData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the features with pre-processing and vectorization done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pre_process(df_train, 'train')\n",
    "data_train = vectorize(df_train)\n",
    "df_val = pre_process(df_val, 'validation')\n",
    "data_val = vectorize(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tranform the data in a way so that it can be fed to Spark's RandomForest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_rdd = data_train.rdd.map(lambda x: LabeledPoint(x.category, MLLibVectors.fromML(x.features)))\n",
    "data_val_rdd = data_val.rdd.map(lambda x: LabeledPoint(x.category, MLLibVectors.fromML(x.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model and make prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForest.trainClassifier(data_train_rdd, numClasses=5, categoricalFeaturesInfo={},\n",
    "                                     numTrees=50, featureSubsetStrategy=\"sqrt\",\n",
    "                                     impurity='gini', maxDepth=3, maxBins=32, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(data_train_rdd.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(data_val_rdd.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labelsAndPredictions = data_train_rdd.map(lambda lp: lp.label).zip(train_predictions)\n",
    "val_labelsAndPredictions = data_val_rdd.map(lambda lp: lp.label).zip(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.875\n",
      "Validation accuracy = 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "actual = np.array(data_train_rdd.map(lambda lp: lp.label).collect())\n",
    "predictions = np.array(train_predictions.collect())\n",
    "train_accuracy = sum(actual == predictions) / float(len(actual))\n",
    "\n",
    "actual = np.array(data_val_rdd.map(lambda lp: lp.label).collect())\n",
    "predictions = np.array(val_predictions.collect())\n",
    "val_accuracy = sum(actual == predictions) / float(len(actual))\n",
    "\n",
    "print('Training accuracy = ' + str(train_accuracy))\n",
    "print('Validation accuracy = ' + str(val_accuracy))\n",
    "# print('Learned classification forest model:')\n",
    "# print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So it turns out the training accuracy is 87.5% and validation accuracy is 83.33%**\n",
    "\n",
    "**Let's also print the classification report with precision, recall and f1-score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "    Computer - Hardware       1.00      1.00      1.00         1\n",
      "Meals and Entertainment       1.00      0.78      0.88         9\n",
      "        Office Supplies       0.00      0.00      0.00         0\n",
      "                 Travel       1.00      1.00      1.00         2\n",
      "\n",
      "            avg / total       1.00      0.83      0.91        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asifiqbal/workspace/envs/t3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_predictions.collect(), df_val.select('category').rdd.map(lambda x:x[0]).collect(), target_names=val_categoroes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
