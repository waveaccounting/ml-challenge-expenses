{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training, validation and employees data files into pandas dataframe\n",
    "training_data = pd.read_csv(\"./training_data_example.csv\")\n",
    "validation_data = pd.read_csv(\"./validation_data_example.csv\")\n",
    "employee_data = pd.read_csv(\"./employee.csv\")\n",
    "\n",
    "training_data = \\\n",
    "pd.merge(training_data, employee_data, how=\"inner\",on=\"employee id\").drop(['employee address','employee name'],axis=1)\n",
    "\n",
    "validation_data = \\\n",
    "pd.merge(validation_data, employee_data, how=\"inner\",on=\"employee id\").drop(['employee address','employee name'],axis=1)\n",
    "\n",
    "# concatenate training and validation data for feature engineering in order to have consistency of columns\n",
    "training_data['type'] = \"training\"\n",
    "validation_data['type'] = \"validation\"\n",
    "combined_data = pd.concat([training_data, validation_data], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       category employee id expense description  \\\n",
      "count                        36          36                  36   \n",
      "unique                        5           7                  20   \n",
      "top     Meals and Entertainment           4              Dinner   \n",
      "freq                         17          15                   8   \n",
      "mean                        NaN         NaN                 NaN   \n",
      "std                         NaN         NaN                 NaN   \n",
      "min                         NaN         NaN                 NaN   \n",
      "25%                         NaN         NaN                 NaN   \n",
      "50%                         NaN         NaN                 NaN   \n",
      "75%                         NaN         NaN                 NaN   \n",
      "max                         NaN         NaN                 NaN   \n",
      "\n",
      "        pre-tax amount      tax name  tax amount   role      type  \n",
      "count        36.000000            36   36.000000     36        36  \n",
      "unique             NaN             2         NaN      4         2  \n",
      "top                NaN  CA Sales tax         NaN  Sales  training  \n",
      "freq               NaN            32         NaN     24        24  \n",
      "mean        423.555556           NaN   52.462222    NaN       NaN  \n",
      "std         754.956250           NaN   94.226685    NaN       NaN  \n",
      "min           4.000000           NaN    0.520000    NaN       NaN  \n",
      "25%          47.500000           NaN    5.850000    NaN       NaN  \n",
      "50%         200.000000           NaN   26.000000    NaN       NaN  \n",
      "75%         251.250000           NaN   32.662500    NaN       NaN  \n",
      "max        4000.000000           NaN  520.000000    NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "# summary of training data \n",
    "combined_data['employee id'] = combined_data['employee id'].astype(str)\n",
    "# drop date column, since we are not considering it as a forecasting problem\n",
    "combined_data = combined_data.drop(['date'],axis=1)\n",
    "print(combined_data.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize real value features \n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "combined_data['pre-tax amount'] = min_max_scaler.\\\n",
    "                            fit_transform(np.array(combined_data['pre-tax amount']).reshape(-1, 1))\n",
    "combined_data['tax amount'] = min_max_scaler.\\\n",
    "                            fit_transform(np.array(combined_data['tax amount']).reshape(-1, 1))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'employee id', 'expense description', 'pre-tax amount',\n",
       "       'tax amount', 'type', 'tax name_NY Sales tax', 'role_Engineer',\n",
       "       'role_IT and Admin', 'role_Sales'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one hot encoded training and validation data for categorical variables 'tax name' and 'role'\n",
    "combined_data = pd.get_dummies(combined_data, columns=['tax name', 'role'], drop_first=True)\n",
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary generated from expense description column\n",
      "Index(['air', 'airplane', 'client', 'coffee', 'computer', 'dinner', 'dropbox',\n",
      "       'family', 'flight', 'hp', 'icloud', 'iphone', 'laptop', 'lunch',\n",
      "       'macbook', 'miami', 'microsoft', 'ny', 'office', 'paper', 'pens',\n",
      "       'potential', 'ride', 'starbucks', 'steve', 'subscription', 'taxi',\n",
      "       'team', 'ticket'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['category', 'employee id', 'expense description', 'pre-tax amount',\n",
       "       'tax amount', 'type', 'tax name_NY Sales tax', 'role_Engineer',\n",
       "       'role_IT and Admin', 'role_Sales', 'expense_desc_stop_removed', 'air',\n",
       "       'airplane', 'client', 'coffee', 'computer', 'dinner', 'dropbox',\n",
       "       'family', 'flight', 'hp', 'icloud', 'iphone', 'laptop', 'lunch',\n",
       "       'macbook', 'miami', 'microsoft', 'ny', 'office', 'paper', 'pens',\n",
       "       'potential', 'ride', 'starbucks', 'steve', 'subscription', 'taxi',\n",
       "       'team', 'ticket'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 'expense description' column generate vocabulary to better parse the description for classification task\n",
    "#pd.get_dummies(training_data, sep=' ', columns=['expense description'], drop_first=True)\n",
    "# first remove stop words to reduce the dimensionality of features thus generated\n",
    "# and convert all words to lower case to disambiguate \n",
    "stop = stopwords.words('english')\n",
    "combined_data['expense_desc_stop_removed'] = combined_data['expense description'].apply(lambda x: ' '.\\\n",
    "                                            join([word.lower() for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n",
    "# then generate vocabulary for all words that occur under expense description column and assign \n",
    "\n",
    "print(\"vocabulary generated from expense description column\")\n",
    "print(combined_data['expense_desc_stop_removed'].str.get_dummies(sep=' ').columns)\n",
    "df_vocab_expenses_ =  combined_data['expense_desc_stop_removed'].str.get_dummies(sep=' ')\n",
    "\n",
    "joined_train_val_ = combined_data.merge(df_vocab_expenses_, how='outer', left_index=True, right_index=True)\n",
    "X_train = joined_train_val_[joined_train_val_.type==\"training\"]\n",
    "X_validation = joined_train_val_[joined_train_val_.type==\"validation\"]\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target/label data by encoding 'category' column\n",
    "Y_train = X_train['category'] \n",
    "le_targets = LabelEncoder()\n",
    "le_targets.fit(Y_train)\n",
    "Y_train = le_targets.transform(Y_train)\n",
    "\n",
    "Y_validation = X_validation['category']\n",
    "le_validation_targets = LabelEncoder()\n",
    "le_validation_targets.fit(Y_validation)\n",
    "Y_validation = le_validation_targets.transform(Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pre-tax amount', 'tax amount', 'tax name_NY Sales tax',\n",
       "       'role_Engineer', 'role_IT and Admin', 'role_Sales', 'air', 'airplane',\n",
       "       'client', 'coffee', 'computer', 'dinner', 'dropbox', 'family', 'flight',\n",
       "       'hp', 'icloud', 'iphone', 'laptop', 'lunch', 'macbook', 'miami',\n",
       "       'microsoft', 'ny', 'office', 'paper', 'pens', 'potential', 'ride',\n",
       "       'starbucks', 'steve', 'subscription', 'taxi', 'team', 'ticket'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the columns not to be used as training feature set\n",
    "drop_cols = ['category', 'employee id', 'expense description', 'expense_desc_stop_removed','type']\n",
    "X_train = X_train.drop(drop_cols, axis=1)\n",
    "X_validation = X_validation.drop(drop_cols, axis=1)\n",
    "X_validation.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04788078, 0.12339354, 0.0255676 , 0.01644852, 0.05863334,\n",
       "       0.04012635, 0.007901  , 0.07450459, 0.02064408, 0.01650796,\n",
       "       0.05401744, 0.11330824, 0.00384615, 0.        , 0.00489718,\n",
       "       0.        , 0.        , 0.02097785, 0.        , 0.        ,\n",
       "       0.00830601, 0.01481128, 0.00761834, 0.02471865, 0.01416276,\n",
       "       0.00848485, 0.        , 0.00771766, 0.12103044, 0.01119617,\n",
       "       0.        , 0.06025599, 0.04579527, 0.02134273, 0.02590522])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Build classifier using SVM as hypothesis and select the hyperparameters using 5 fold cross validation.\n",
    "    Since, we have very limited number of samples SVM (support vector machine) would be a good choice as it has been\n",
    "    proven to work well in such situation. \n",
    "    There's a chance for model to overfit to training data, so it's important to tune regularization parameter 'C'\n",
    "    cross validation to avoid overfitting as its often the case from small datasets\n",
    "    'gamma' is the rbf kernel parameter that dictates the infleunce of a single parameter\n",
    "\"\"\"\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuning_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4],'C': [1, 10, 100]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
    "scores = ['precision', 'recall', 'f1']\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.128 (+/-0.171) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.128 (+/-0.171) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.128 (+/-0.171) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.386 (+/-0.642) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.128 (+/-0.171) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.128 (+/-0.171) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.602 (+/-0.229) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.386 (+/-0.642) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.128 (+/-0.171) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.602 (+/-0.229) for {'C': 1, 'kernel': 'linear'}\n",
      "0.602 (+/-0.229) for {'C': 10, 'kernel': 'linear'}\n",
      "0.602 (+/-0.229) for {'C': 100, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full validation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.00      0.00      0.00         7\n",
      "          2       0.22      1.00      0.36         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.25      0.14        12\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.281 (+/-0.187) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.281 (+/-0.187) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.281 (+/-0.187) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.499 (+/-0.548) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.281 (+/-0.187) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.281 (+/-0.187) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.647 (+/-0.313) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.499 (+/-0.548) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.281 (+/-0.187) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.647 (+/-0.313) for {'C': 1, 'kernel': 'linear'}\n",
      "0.647 (+/-0.313) for {'C': 10, 'kernel': 'linear'}\n",
      "0.647 (+/-0.313) for {'C': 100, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full validation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.00      0.00      0.00         7\n",
      "          2       0.22      1.00      0.36         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.25      0.14        12\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.172 (+/-0.193) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.172 (+/-0.193) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.172 (+/-0.193) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.421 (+/-0.623) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.172 (+/-0.193) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.172 (+/-0.193) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.612 (+/-0.251) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.421 (+/-0.623) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.172 (+/-0.193) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.612 (+/-0.251) for {'C': 1, 'kernel': 'linear'}\n",
      "0.612 (+/-0.251) for {'C': 10, 'kernel': 'linear'}\n",
      "0.612 (+/-0.251) for {'C': 100, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full validation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.00      0.00      0.00         7\n",
      "          2       0.22      1.00      0.36         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.25      0.14        12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuning_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full validation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = Y_validation, clf.predict(X_validation)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.117 (+/-0.182) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.137 (+/-0.184) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.128 (+/-0.171) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "0.119 (+/-0.186) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.128 (+/-0.171) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.128 (+/-0.171) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.236 (+/-0.361) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.235 (+/-0.459) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.185 (+/-0.258) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.273 (+/-0.366) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.307 (+/-0.349) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.347 (+/-0.479) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "0.117 (+/-0.182) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.128 (+/-0.171) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.128 (+/-0.171) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.151 (+/-0.266) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.175 (+/-0.239) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.164 (+/-0.232) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.375 (+/-0.439) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.307 (+/-0.496) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.342 (+/-0.441) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full validation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.00      0.00      0.00         7\n",
      "          2       0.22      1.00      0.36         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.25      0.14        12\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.260 (+/-0.103) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.281 (+/-0.187) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "0.281 (+/-0.187) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.281 (+/-0.187) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.281 (+/-0.187) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.275 (+/-0.120) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.291 (+/-0.237) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.291 (+/-0.237) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.364 (+/-0.252) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.492 (+/-0.419) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.492 (+/-0.419) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "0.281 (+/-0.187) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.281 (+/-0.187) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.281 (+/-0.187) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.452 (+/-0.219) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.333 (+/-0.252) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.291 (+/-0.237) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.465 (+/-0.493) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.465 (+/-0.493) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.452 (+/-0.442) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full validation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.00      0.00      0.00         7\n",
      "          2       0.22      1.00      0.36         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.25      0.14        12\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.136 (+/-0.230) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.265 (+/-0.495) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.172 (+/-0.193) for {'max_features': 1, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "0.172 (+/-0.193) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.172 (+/-0.193) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.172 (+/-0.193) for {'max_features': 0.3, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.372 (+/-0.614) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.302 (+/-0.415) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.230 (+/-0.270) for {'max_features': 0.3, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.312 (+/-0.420) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.490 (+/-0.306) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.379 (+/-0.490) for {'max_features': 0.3, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "0.128 (+/-0.229) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 10}\n",
      "0.172 (+/-0.193) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 100}\n",
      "0.172 (+/-0.193) for {'max_features': 0.75, 'max_samples': 1, 'n_estimators': 500}\n",
      "0.418 (+/-0.333) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 10}\n",
      "0.263 (+/-0.387) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 100}\n",
      "0.200 (+/-0.239) for {'max_features': 0.75, 'max_samples': 0.3, 'n_estimators': 500}\n",
      "0.432 (+/-0.376) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 10}\n",
      "0.348 (+/-0.503) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 100}\n",
      "0.384 (+/-0.441) for {'max_features': 0.75, 'max_samples': 0.7, 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full validation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.00      0.00      0.00         7\n",
      "          2       0.22      1.00      0.36         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.25      0.14        12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Since, the data is limited its a good idea to try ensemble approach known as Bagging which trains mulitple models \n",
    "    randomly selected proportion of dataset in terms of features and samples. \n",
    "    This often helps counteract the data limitation and curse ofdimensionality problems. It has a similar effect as \n",
    "    oversampling.\n",
    "\"\"\"\n",
    "\n",
    "# hypothesis: Bagging Classifier with decision tree as base estimator\n",
    "print(\"classification model with Bagging Classifier\")\n",
    "# Set the parameters by cross-validation\n",
    "tuning_parameters = [{'n_estimators': [10,100,500], \n",
    "                      'max_samples':[1,0.3,0.7], \n",
    "                      'max_features':[1,0.3,0.75]},\n",
    "                     ]\n",
    "scores = ['precision', 'recall', 'f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(BaggingClassifier(), tuning_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full validation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = Y_validation, clf.predict(X_validation)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 10)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    In order to discriminate between business and personal expenses when no labeled data is present is an unsupervised\n",
    "    learning problem. \n",
    "    To tackle this, first features needs to be prepared that are apt for a clustering algorithm input. \n",
    "    There are 2 real valued features 'pre-tax amount' and 'tax amount' that are used along with the categorical features\n",
    "    'tax name', 'role','category'. The categorical features need to binarized as we did with classification model in \n",
    "    order to map them to real values. \n",
    "\n",
    "    Also note 'expense description' can be vital for the clustering algorithm but it leads a high dimensional \n",
    "    feature space and with such a small dataset it wouldn't be feasible. But for a sufficiently large dataset, \n",
    "    same approach can be followed as in classification feature engineering ie generate vocabulary and binarize.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "training_data_clustering = training_data\n",
    "training_data_clustering['pre-tax amount'] = min_max_scaler.\\\n",
    "                            fit_transform(np.array(training_data['pre-tax amount']).reshape(-1, 1))\n",
    "training_data_clustering['tax amount'] = min_max_scaler.\\\n",
    "                            fit_transform(np.array(training_data['tax amount']).reshape(-1, 1)) \n",
    "training_data_clustering = pd.get_dummies(training_data_clustering, columns=['tax name', 'role','category'], \\\n",
    "                                          drop_first=True)\n",
    "# drop columns \n",
    "training_data_clustering = training_data_clustering.drop(['employee id','date','type','expense description'],axis=1).\\\n",
    "                                    astype('float')\n",
    "X_train_clustering = training_data_clustering.values\n",
    "\n",
    "X_train_clustering.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note:  cluster_index column shows the cluster a data point belongs to\n",
      "\n",
      "          date                 category  employee id  \\\n",
      "0    11/1/2016                   Travel            7   \n",
      "1   11/21/2016  Meals and Entertainment            7   \n",
      "2   10/12/2016      Computer - Hardware            7   \n",
      "3   11/15/2016  Meals and Entertainment            1   \n",
      "4   12/11/2016      Computer - Software            1   \n",
      "5    9/18/2016                   Travel            1   \n",
      "6    11/7/2016                   Travel            1   \n",
      "7   11/30/2016      Computer - Hardware            3   \n",
      "8   11/14/2016      Computer - Software            3   \n",
      "9    11/3/2016      Computer - Software            3   \n",
      "10   9/30/2016          Office Supplies            3   \n",
      "11   11/6/2016      Computer - Software            4   \n",
      "12  11/12/2016                   Travel            4   \n",
      "13  12/30/2016  Meals and Entertainment            4   \n",
      "14  12/15/2016  Meals and Entertainment            4   \n",
      "15   12/1/2016  Meals and Entertainment            4   \n",
      "16   12/8/2016  Meals and Entertainment            4   \n",
      "17  12/31/2016  Meals and Entertainment            4   \n",
      "18   12/9/2016  Meals and Entertainment            4   \n",
      "19   12/9/2016  Meals and Entertainment            6   \n",
      "20   10/4/2016                   Travel            6   \n",
      "21   11/6/2016      Computer - Hardware            6   \n",
      "22  12/18/2016                   Travel            6   \n",
      "23   12/3/2016  Meals and Entertainment            5   \n",
      "\n",
      "             expense description  pre-tax amount      tax name  tax amount  \\\n",
      "0                      Taxi ride        0.018045  NY Sales tax    0.015580   \n",
      "1                  Client dinner        0.098246  NY Sales tax    0.088595   \n",
      "2           Macbook Air Computer        1.000000  NY Sales tax    0.909554   \n",
      "3                     Team lunch        0.115789  CA Sales tax    0.154412   \n",
      "4            iCloud Subscription        0.005514  CA Sales tax    0.007353   \n",
      "5                      Taxi ride        0.028070  CA Sales tax    0.037433   \n",
      "6          Airplane ticket to NY        0.098246  CA Sales tax    0.131016   \n",
      "7             HP Laptop Computer        0.498747  CA Sales tax    0.665107   \n",
      "8               Microsoft Office        0.448622  CA Sales tax    0.598262   \n",
      "9           Dropbox Subscription        0.023058  CA Sales tax    0.030749   \n",
      "10                         Paper        0.098246  CA Sales tax    0.131016   \n",
      "11          Dropbox Subscription        0.023058  CA Sales tax    0.030749   \n",
      "12                     Taxi ride        0.113283  CA Sales tax    0.151070   \n",
      "13  Dinner with potential client        0.098246  CA Sales tax    0.131016   \n",
      "14            Dinner with client        0.098246  CA Sales tax    0.131016   \n",
      "15                        Dinner        0.103258  CA Sales tax    0.137701   \n",
      "16                        Dinner        0.088221  CA Sales tax    0.117647   \n",
      "17                        Dinner        0.499248  CA Sales tax    0.665775   \n",
      "18                        Dinner        0.013033  CA Sales tax    0.017380   \n",
      "19             Coffee with Steve        0.148371  CA Sales tax    0.197861   \n",
      "20               Flight to Miami        0.098246  CA Sales tax    0.131016   \n",
      "21                        iPhone        0.098246  CA Sales tax    0.131016   \n",
      "22         Airplane ticket to NY        0.749875  CA Sales tax    1.000000   \n",
      "23              Starbucks coffee        0.000000  CA Sales tax    0.000000   \n",
      "\n",
      "            role  cluster_index  \n",
      "0          Sales              0  \n",
      "1          Sales              1  \n",
      "2          Sales              0  \n",
      "3            CEO              1  \n",
      "4            CEO              0  \n",
      "5            CEO              0  \n",
      "6            CEO              0  \n",
      "7   IT and Admin              0  \n",
      "8   IT and Admin              0  \n",
      "9   IT and Admin              0  \n",
      "10  IT and Admin              0  \n",
      "11         Sales              0  \n",
      "12         Sales              0  \n",
      "13         Sales              1  \n",
      "14         Sales              1  \n",
      "15         Sales              1  \n",
      "16         Sales              1  \n",
      "17         Sales              1  \n",
      "18         Sales              1  \n",
      "19         Sales              1  \n",
      "20         Sales              0  \n",
      "21         Sales              0  \n",
      "22         Sales              0  \n",
      "23      Engineer              1  \n"
     ]
    }
   ],
   "source": [
    "# kmeans Clustering algorithm is used with 2 clusters\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_train_clustering)\n",
    "cluster_index = kmeans.labels_\n",
    "training_data['cluster_index'] = kmeans.labels_\n",
    "# cluster index is an arbitrary attribute that distinguishes one cluster from the other\n",
    "print(\"note:  cluster_index column shows the cluster a data point belongs to\")\n",
    "print()\n",
    "print(training_data.drop(['type'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                 category  employee id  \\\n",
      "0    11/1/2016                   Travel            7   \n",
      "1   11/21/2016  Meals and Entertainment            7   \n",
      "2   10/12/2016      Computer - Hardware            7   \n",
      "3   11/15/2016  Meals and Entertainment            1   \n",
      "4   12/11/2016      Computer - Software            1   \n",
      "5    9/18/2016                   Travel            1   \n",
      "6    11/7/2016                   Travel            1   \n",
      "7   11/30/2016      Computer - Hardware            3   \n",
      "8   11/14/2016      Computer - Software            3   \n",
      "9    11/3/2016      Computer - Software            3   \n",
      "10   9/30/2016          Office Supplies            3   \n",
      "11   11/6/2016      Computer - Software            4   \n",
      "12  11/12/2016                   Travel            4   \n",
      "13  12/30/2016  Meals and Entertainment            4   \n",
      "14  12/15/2016  Meals and Entertainment            4   \n",
      "15   12/1/2016  Meals and Entertainment            4   \n",
      "16   12/8/2016  Meals and Entertainment            4   \n",
      "17  12/31/2016  Meals and Entertainment            4   \n",
      "18   12/9/2016  Meals and Entertainment            4   \n",
      "19   12/9/2016  Meals and Entertainment            6   \n",
      "20   10/4/2016                   Travel            6   \n",
      "21   11/6/2016      Computer - Hardware            6   \n",
      "22  12/18/2016                   Travel            6   \n",
      "23   12/3/2016  Meals and Entertainment            5   \n",
      "\n",
      "             expense description  pre-tax amount      tax name  tax amount  \\\n",
      "0                      Taxi ride        0.018045  NY Sales tax    0.015580   \n",
      "1                  Client dinner        0.098246  NY Sales tax    0.088595   \n",
      "2           Macbook Air Computer        1.000000  NY Sales tax    0.909554   \n",
      "3                     Team lunch        0.115789  CA Sales tax    0.154412   \n",
      "4            iCloud Subscription        0.005514  CA Sales tax    0.007353   \n",
      "5                      Taxi ride        0.028070  CA Sales tax    0.037433   \n",
      "6          Airplane ticket to NY        0.098246  CA Sales tax    0.131016   \n",
      "7             HP Laptop Computer        0.498747  CA Sales tax    0.665107   \n",
      "8               Microsoft Office        0.448622  CA Sales tax    0.598262   \n",
      "9           Dropbox Subscription        0.023058  CA Sales tax    0.030749   \n",
      "10                         Paper        0.098246  CA Sales tax    0.131016   \n",
      "11          Dropbox Subscription        0.023058  CA Sales tax    0.030749   \n",
      "12                     Taxi ride        0.113283  CA Sales tax    0.151070   \n",
      "13  Dinner with potential client        0.098246  CA Sales tax    0.131016   \n",
      "14            Dinner with client        0.098246  CA Sales tax    0.131016   \n",
      "15                        Dinner        0.103258  CA Sales tax    0.137701   \n",
      "16                        Dinner        0.088221  CA Sales tax    0.117647   \n",
      "17                        Dinner        0.499248  CA Sales tax    0.665775   \n",
      "18                        Dinner        0.013033  CA Sales tax    0.017380   \n",
      "19             Coffee with Steve        0.148371  CA Sales tax    0.197861   \n",
      "20               Flight to Miami        0.098246  CA Sales tax    0.131016   \n",
      "21                        iPhone        0.098246  CA Sales tax    0.131016   \n",
      "22         Airplane ticket to NY        0.749875  CA Sales tax    1.000000   \n",
      "23              Starbucks coffee        0.000000  CA Sales tax    0.000000   \n",
      "\n",
      "            role      type  cluster_index  \n",
      "0          Sales  training              0  \n",
      "1          Sales  training              1  \n",
      "2          Sales  training              0  \n",
      "3            CEO  training              1  \n",
      "4            CEO  training              0  \n",
      "5            CEO  training              0  \n",
      "6            CEO  training              0  \n",
      "7   IT and Admin  training              0  \n",
      "8   IT and Admin  training              0  \n",
      "9   IT and Admin  training              0  \n",
      "10  IT and Admin  training              0  \n",
      "11         Sales  training              0  \n",
      "12         Sales  training              0  \n",
      "13         Sales  training              1  \n",
      "14         Sales  training              1  \n",
      "15         Sales  training              1  \n",
      "16         Sales  training              1  \n",
      "17         Sales  training              1  \n",
      "18         Sales  training              1  \n",
      "19         Sales  training              1  \n",
      "20         Sales  training              0  \n",
      "21         Sales  training              0  \n",
      "22         Sales  training              0  \n",
      "23      Engineer  training              1  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
